{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0372867c-a027-46d8-88cf-ab64b116cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import enchant\n",
    "import joblib\n",
    "import os\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a610bf7c-3450-4b3d-8b7f-989bd05d256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"davidson\" : pd.read_csv(\"literature\\datasets\\dt\\labeled_data_all_2classes_only.csv\", index_col=0),\n",
    "    \"hateval\" : pd.read_csv(\"literature\\\\datasets\\\\hateval\\\\train_en.tsv\", sep='\\t', index_col=0),\n",
    "    \"ethos\" : pd.read_csv(\"literature\\\\datasets\\\\ethos\\\\Ethos_Dataset_Binary.csv\", sep=';')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b711b8-f89b-4a71-9dc2-4647a95f22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocess\n",
    "datasets[\"davidson\"] = datasets[\"davidson\"].rename({\"tweet\":\"text\"}, axis=1)\n",
    "datasets[\"davidson\"]['class'] = datasets[\"davidson\"]['class'].replace({0: 1, 2: 0})\n",
    "\n",
    "datasets[\"hateval\"] = datasets[\"hateval\"].rename({\"HS\":\"class\"}, axis=1)\n",
    "\n",
    "datasets[\"ethos\"] = datasets[\"ethos\"].rename({\"comment\":\"text\", \"isHate\":\"class\"}, axis=1)\n",
    "datasets[\"ethos\"]['class'] = datasets[\"ethos\"]['class'].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb48a52-e727-4a33-af26-9a40eff965cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    df_pm = data.lower()\n",
    "    return df_pm\n",
    "def sep_rem(data):\n",
    "    df_pm = data\n",
    "    # df_pm = re.sub(r\"[-()\\\"/;:<>{}`+=~|.!?,]\", \"\", df_pm)\n",
    "    df_pm = re.sub(r\"[^a-zA-Z0-9#@ ]\",\"\",df_pm)\n",
    "    return df_pm.strip()\n",
    "def remove_hashtag(data):\n",
    "    df_pm = re.sub(r\"#\\S+\", \"\", data)\n",
    "    return df_pm.strip()\n",
    "def remove_mentions(data):\n",
    "    data = re.sub(r\"@\\S+\", \"\", data)\n",
    "    return data.strip()\n",
    "def remove_NCR(data):\n",
    "    data = re.sub(r\"&#[0-9]+;|&#x[0-9a-fA-F]+;|&[a-zA-Z]+\", \"\", data)\n",
    "    return data.strip()\n",
    "def remove_RT(data):\n",
    "    data = re.sub(r\"(^|\\s)rt\\s\", \"\", data)\n",
    "    return data.strip()\n",
    "def remove_links(data):\n",
    "    data = re.sub(r\"https?://(?:[\\w./])+\", \" \", data)\n",
    "    data = re.sub(r\"http?://(?:[\\w./&#])+\", \" \", data)\n",
    "    return data.strip()\n",
    "def remove_spaces(data):\n",
    "    data = re.sub(r\" +\", \" \", data)\n",
    "    return data.strip()\n",
    "    \n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "def spell_check_and_replace(text):\n",
    "    # Tokenize the text into words\n",
    "    words = np.array(text.split())\n",
    "    \n",
    "    # Create an empty array to store the misspelled mask\n",
    "    misspelled_mask = np.zeros(len(words), dtype=bool)\n",
    "    \n",
    "    # Iterate through each word\n",
    "    for i, word in enumerate(words):\n",
    "        # Check if the word is misspelled\n",
    "        if not d.check(word):\n",
    "            # Update the misspelled mask\n",
    "            misspelled_mask[i] = True\n",
    "    \n",
    "    # Get suggestions for misspelled words\n",
    "    misspelled_words = words[misspelled_mask]\n",
    "    suggestions = [d.suggest(word) for word in misspelled_words]\n",
    "    \n",
    "    # Replace misspelled words with the first suggestion\n",
    "    for i, suggestion_list in enumerate(suggestions):\n",
    "        if suggestion_list:\n",
    "            misspelled_word_index = np.where(words == misspelled_words[i])[0][0]\n",
    "            words[misspelled_word_index] = suggestion_list[0]\n",
    "    \n",
    "    # Join the corrected words back into a single string\n",
    "    corrected_text = ' '.join(words)\n",
    "    \n",
    "    return corrected_text\n",
    "def process_data(df):\n",
    "    processed_column = df['text']\n",
    "    processed_column = processed_column.apply(lambda x:pre_process(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_links(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_NCR(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_hashtag(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_mentions(x))\n",
    "    processed_column = processed_column.apply(lambda x:sep_rem(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_RT(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_spaces(x))\n",
    "    # processed_column = processed_column.apply(lambda x:spell_check_and_replace(x))\n",
    "    df[\"clean\"] = processed_column\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea99545-4626-45c9-976a-a1d63b51453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretrained_model = api.load('word2vec-google-news-300')\n",
    "word_indices = list(pretrained_model.key_to_index.keys())\n",
    "word_freq = {word: pretrained_model.get_vecattr(word, \"count\") for word in word_indices}\n",
    "\n",
    "def text_to_vectors(text):\n",
    "    # Tokenize the text (assuming it's already tokenized)\n",
    "    tokens = text.split()  # You may need a different tokenization method depending on your data\n",
    "    \n",
    "    # Get word vectors for each token\n",
    "    word_vectors = [pretrained_model[token] for token in tokens if token in pretrained_model]\n",
    "\n",
    "    if not word_vectors:\n",
    "        # Return zero-filled array of the desired shape\n",
    "        return np.zeros((100, 300), dtype=np.float32)\n",
    "    \n",
    "    # Pad sequence\n",
    "    max_length = 100\n",
    "    padded_vectors = np.zeros((max_length, 300), dtype=np.float32)\n",
    "    num_vectors = min(len(word_vectors), max_length)\n",
    "    padded_vectors[:num_vectors] = word_vectors[:max_length]\n",
    "    \n",
    "    return padded_vectors\n",
    "\n",
    "# Feature Representation\n",
    "def feature_rep(df, feature_type=\"TFIDF\"):\n",
    "    start_time = time.time()\n",
    "    if feature_type == \"TFIDF\": # TFIDF\n",
    "        documents = df[\"clean\"].tolist()\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(lowercase=False,ngram_range=(1, 3),max_features=10000,min_df=5,max_df=0.501)\n",
    "        X = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    elif feature_type == \"BOW\": # Bag of words\n",
    "        vectorizer = CountVectorizer()\n",
    "        documents = df[\"clean\"].tolist()\n",
    "    \n",
    "        bow_matrix = vectorizer.fit_transform(documents)\n",
    "        X = bow_matrix.toarray()\n",
    "    \n",
    "        vocabulary = vectorizer.get_feature_names_out()\n",
    "    elif feature_type == \"W2V\": # Word2Vec\n",
    "        df['tokenized_text'] = df['clean'].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        # Initialize Word2Vec model with parameters\n",
    "        w2v_model = Word2Vec(df['tokenized_text'], vector_size=300, window=5, min_count=1, workers=4)\n",
    "        \n",
    "        # Build vocabulary and update it with word frequency\n",
    "        w2v_model.build_vocab_from_freq(word_freq, update=True)\n",
    "        w2v_model.wv.vectors_lockf = np.ones(len(w2v_model.wv), dtype=np.dtype(float))\n",
    "        \n",
    "        # Load pretrained Word2Vec vectors and intersect with the current model\n",
    "        pretrained_model_path = api.info('word2vec-google-news-300')['file_name']\n",
    "        pretrained_model_path_full = \"C:\\\\Users\\\\Portul\\\\gensim-data\\\\word2vec-google-news-300\\\\\" + pretrained_model_path\n",
    "        w2v_model.wv.intersect_word2vec_format(pretrained_model_path_full, binary=True, lockf=1.0)\n",
    "        \n",
    "        # Apply text_to_vectors function to convert text to Word2Vec vectors\n",
    "        df['text_vector'] = df['clean'].apply(text_to_vectors)\n",
    "        \n",
    "        # Average the word vectors for each document\n",
    "        document_vectors = df['text_vector'].apply(lambda x: np.mean(x, axis=0) if len(x) > 0 else np.zeros(300))\n",
    "        \n",
    "        # Convert document vectors to numpy array\n",
    "        X = np.vstack(document_vectors)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Feature Type:\", feature_type)\n",
    "    print(\"Feature Rep Elapsed time:\", round(elapsed_time,2), \"seconds\")\n",
    "    print(\"\")\n",
    "    return X, df[\"class\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23e5c0b-a3ea-4c45-8c90-deefab0dae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X, y, model_type=\"SVM\"):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    \n",
    "    f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "    cv_scores = []\n",
    "    elapsed_times = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "        \n",
    "    if model_type == \"SVM\":\n",
    "        print(\"Model Type:\", model_type)\n",
    "        model = LinearSVC(class_weight='balanced', C=0.01, penalty='l2', loss='squared_hinge', multi_class='ovr')\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5, scoring=f1_scorer )\n",
    "    \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    elif model_type == \"NB\":\n",
    "        print(\"Model Type:\", model_type)\n",
    "        model = MultinomialNB()\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5, scoring=f1_scorer )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    elif model_type == \"LR\":\n",
    "        print(\"Model Type:\", model_type)\n",
    "        model = LogisticRegression(class_weight='balanced', penalty=\"l2\")\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5, scoring=f1_scorer )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = time.time()\n",
    "        \n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, 'temp_model.pkl')\n",
    "    \n",
    "    # Get the size of the saved file\n",
    "    model_file_size = os.path.getsize('temp_model.pkl')\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    pred = model.predict(X_val)\n",
    "\n",
    "    report = classification_report(y_val, pred, output_dict=True)\n",
    "\n",
    "    \n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean CV F1:\", cv_scores.mean())\n",
    "    print(\"Standard Deviation of CV F1:\", cv_scores.std())\n",
    "    \n",
    "    print(classification_report(y_val, pred))\n",
    "    print(f\"{round(report['1']['precision'], 2)}/{round(report['1']['recall'], 2)}/{round(report['1']['f1-score'], 2)}/{round(report['macro avg']['f1-score'], 2)}/{round(cv_scores.mean(), 2)}\")\n",
    "    print(\"Elapsed time:\", round(elapsed_time,3), \"seconds\")\n",
    "    print(\"Model file size:\", model_file_size, \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e7e2ab-18aa-43c2-a057-23e49e5f0357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  davidson\n",
      "\n",
      "Feature Type: TFIDF\n",
      "Feature Rep Elapsed time: 1.19 seconds\n",
      "\n",
      "Model Type: SVM\n",
      "Cross-Validation Scores: [0.88857946 0.8905991  0.91636755 0.91723893 0.91323786]\n",
      "Mean CV F1: 0.9052045779364535\n",
      "Standard Deviation of CV F1: 0.012834997317196705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.94      7023\n",
      "           1       0.27      0.60      0.37       412\n",
      "\n",
      "    accuracy                           0.89      7435\n",
      "   macro avg       0.62      0.75      0.66      7435\n",
      "weighted avg       0.94      0.89      0.91      7435\n",
      "\n",
      "0.27/0.6/0.37/0.66/0.91\n",
      "Elapsed time: 0.017 seconds\n",
      "Model file size: 80753 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Portul\\.conda\\envs\\ml_env3.10\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Portul\\.conda\\envs\\ml_env3.10\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Portul\\.conda\\envs\\ml_env3.10\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Portul\\.conda\\envs\\ml_env3.10\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Portul\\.conda\\envs\\ml_env3.10\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Portul\\.conda\\envs\\ml_env3.10\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = \"davidson\"\n",
    "print(\"Dataset: \", dataset)\n",
    "print(\"\")\n",
    "\n",
    "df = datasets[dataset]\n",
    "df = process_data(df)\n",
    "X, y = feature_rep(df, feature_type=\"TFIDF\")\n",
    "train_classifier(X, y, model_type=\"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e1371-068a-4b82-8941-f59b3aa257ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
