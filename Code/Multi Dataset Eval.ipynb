{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fc3d76-d632-436d-9be1-ad8d08f2bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Embedding, Dense, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten, InputLayer, Input, Dropout, Concatenate, GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a1edcd-9c32-415c-90d3-ba6264803c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"davidson\" : pd.read_csv(\"datasets\\model_training\\davidson_p.csv\"),\n",
    "    \"hateval\" : pd.read_csv(\"datasets\\model_training\\hateval_p.csv\"),\n",
    "    \"ethos\" : pd.read_csv(\"datasets\\model_training\\ethos_p.csv\"),\n",
    "    \"jigsaw\": pd.read_csv(\"datasets\\model_training\\jigsaw_p.csv\"),\n",
    "    \"qian\": pd.read_csv(\"datasets\\model_training\\qian_p.csv\")\n",
    "}\n",
    "\n",
    "datasets_r = {\n",
    "    \"davidson\" : pd.read_csv(\"datasets\\model_training\\davidson_r.csv\"),\n",
    "    \"hateval\" : pd.read_csv(\"datasets\\model_training\\hateval_r.csv\"),\n",
    "    \"ethos\" : pd.read_csv(\"datasets\\model_training\\ethos_r.csv\"),\n",
    "    \"jigsaw\": pd.read_csv(\"datasets\\model_training\\jigsaw_r.csv\"),\n",
    "    \"qian\": pd.read_csv(\"datasets\\model_training\\qian_r.csv\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45943704-02be-45ae-9018-0e62b59fa300",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train = {\n",
    "    \"davidson\" : pd.read_csv(\"datasets\\model_training\\ensemble\\davidson_train.csv\"),\n",
    "    \"hateval\" : pd.read_csv(\"datasets\\model_training\\ensemble\\hateval_train.csv\"),\n",
    "    \"ethos\" : pd.read_csv(\"datasets\\model_training\\ensemble\\ethos_train.csv\"),\n",
    "    \"jigsaw\": pd.read_csv(\"datasets\\model_training\\ensemble\\jigsaw_train.csv\"),\n",
    "    \"qian\": pd.read_csv(\"datasets\\model_training\\ensemble\\qian_train.csv\"),\n",
    "    \"combined\": pd.read_csv(\"datasets\\model_training\\ensemble\\combined_train.csv\")\n",
    "}\n",
    "datasets_test = {\n",
    "    \"davidson\" : pd.read_csv(\"datasets\\model_training\\ensemble\\davidson_test.csv\"),\n",
    "    \"hateval\" : pd.read_csv(\"datasets\\model_training\\ensemble\\hateval_test.csv\"),\n",
    "    \"ethos\" : pd.read_csv(\"datasets\\model_training\\ensemble\\ethos_test.csv\"),\n",
    "    \"jigsaw\": pd.read_csv(\"datasets\\model_training\\ensemble\\jigsaw_test.csv\"),\n",
    "    \"qian\": pd.read_csv(\"datasets\\model_training\\ensemble\\qian_test.csv\"),\n",
    "    \"combined\": pd.read_csv(\"datasets\\model_training\\ensemble\\combined_test.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159044e4-7472-47cd-89c0-494dacf12835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractXy(df):\n",
    "    X = df['text'].astype(\"str\").tolist()\n",
    "    X = np.array(X).reshape(len(X), 1)\n",
    "    y = pd.get_dummies(df['class']).values.astype(int)\n",
    "    return X, y\n",
    "\n",
    "def feature_rep(df):\n",
    "    tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\"\n",
    "    tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\"\n",
    "    \n",
    "    X = df['text'].astype(\"str\").tolist()\n",
    "    X = np.array(X).reshape(len(X), 1)\n",
    "    y = pd.get_dummies(df['class']).values.astype(int)\n",
    "    input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    \n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing', trainable=False)\n",
    "    encoder_inputs = preprocessing_layer(input_layer)\n",
    "    \n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder')\n",
    "    feature_rep_end = encoder(encoder_inputs)['sequence_output']\n",
    "    \n",
    "    return input_layer,feature_rep_end, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7cd869-c539-489f-bdf8-4faa991cc2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GRU_Model: # Model from Zhang et al.\n",
    "    def __init__(self, input_layer, feature_rep_end):\n",
    "        self.input_layer = input_layer\n",
    "        self.feature_rep_end = feature_rep_end\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        conv_layer = Conv1D(filters=100, kernel_size=4, activation='relu')(self.feature_rep_end)\n",
    "        max_pool = MaxPooling1D(pool_size=4)(conv_layer)\n",
    "        gru = GRU(100, return_sequences=True)(max_pool)\n",
    "        global_pool = GlobalMaxPooling1D()(gru)\n",
    "        dense1 = Dense(2, activation='softmax',kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))(global_pool)\n",
    "        model = Model(inputs=self.input_layer, outputs=dense1)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43dd70f-3e88-4ee6-b77e-03ece246e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(input_layer, feature_rep_end, X_train, y_train, X_val, y_val, batch_size=128, epochs=30, patience=3):\n",
    "    model_class = CNN_GRU_Model(input_layer, feature_rep_end)\n",
    "    model = model_class.build_model()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the training data\n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_train, axis=1)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(report)\n",
    "\n",
    "    return model\n",
    "\n",
    "def eval(model, X_val, y_val):\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "    precision = precision_score(y_val, y_pred, average='binary')\n",
    "    recall = recall_score(y_val, y_pred, average='binary')\n",
    "    f1_macro_score = f1_score(y_val, y_pred, average='macro')\n",
    "    f1_weighted_score = f1_score(y_val, y_pred, average='weighted')\n",
    "    report = classification_report(y_val, y_pred)\n",
    "\n",
    "    print(\"Precision for Hate Class:\", precision)\n",
    "    print(\"Recall for Hate Class:\", recall)\n",
    "    print(\"F1 Macro\", f1_macro_score)\n",
    "    print(\"F1 Weighted\", f1_weighted_score)\n",
    "    print(round(precision,2), \"/\",round(recall,2), \"/\", round(f1_macro_score,2), \"/\", round(f1_weighted_score,2))\n",
    "    print(report)\n",
    "    return round(precision,2), round(recall,2), round(f1_macro_score,2), round(f1_weighted_score,2), report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88307821-79e9-419c-828e-19750a34a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resamp(df):\n",
    "    X = df.drop('class', axis=1)  # Features\n",
    "    y = df['class']  # Target variable\n",
    "    \n",
    "    # Initialize the RandomOverSampler\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    \n",
    "    # Perform the oversampling\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "    X_resampled[\"class\"] = y_resampled\n",
    "    return X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f3a7c-05b1-4718-a3b6-90048b558c03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7adf95d-e0a8-47a3-b548-c56b24ff1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw_hate = datasets[\"jigsaw\"][datasets[\"jigsaw\"][\"class\"] == 1]\n",
    "jigsaw_normal = datasets[\"jigsaw\"][datasets[\"jigsaw\"][\"class\"] == 0]\n",
    "jigsaw_small = pd.concat([jigsaw_normal.sample(n=10000),jigsaw_hate], ignore_index=True)\n",
    "jigsaw_small_r = resamp(jigsaw_small)\n",
    "datasets[\"jigsaw_small\"] = jigsaw_small\n",
    "datasets_r[\"jigsaw_small\"] = jigsaw_small_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b80f475f-ceb4-44da-b1a2-8dbfd5aec40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qian_hate = datasets[\"qian\"][datasets[\"qian\"][\"class\"] == 1]\n",
    "qian_normal = datasets[\"qian\"][datasets[\"qian\"][\"class\"] == 0]\n",
    "qian_small = pd.concat([qian_normal.sample(n=5000), qian_hate.sample(n=5000)], ignore_index=True)\n",
    "# qian_small_r = resamp(qian_small)\n",
    "datasets[\"qian_small\"] = qian_small\n",
    "# datasets_r[\"qian_small\"] = qian_small_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33b8d5-78b2-43e6-942a-775942473898",
   "metadata": {},
   "source": [
    "## Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caeed520-60d3-40ab-a612-6570ac16c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9026a573-556d-45ab-84b3-14cfdee8b0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 405s 2s/step - loss: 0.5754 - accuracy: 0.8088 - val_loss: 0.4501 - val_accuracy: 0.8407\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 410s 2s/step - loss: 0.4256 - accuracy: 0.8472 - val_loss: 0.4199 - val_accuracy: 0.8445\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 418s 2s/step - loss: 0.3987 - accuracy: 0.8591 - val_loss: 0.4154 - val_accuracy: 0.8484\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 418s 2s/step - loss: 0.3719 - accuracy: 0.8743 - val_loss: 0.4260 - val_accuracy: 0.8330\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 422s 2s/step - loss: 0.3425 - accuracy: 0.8905 - val_loss: 0.4238 - val_accuracy: 0.8418\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.9079Restoring model weights from the end of the best epoch: 3.\n",
      "236/236 [==============================] - 424s 2s/step - loss: 0.3128 - accuracy: 0.9079 - val_loss: 0.4272 - val_accuracy: 0.8425\n",
      "Epoch 6: early stopping\n",
      "941/941 [==============================] - 222s 235ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     23253\n",
      "           1       0.83      0.53      0.65      6846\n",
      "\n",
      "    accuracy                           0.87     30099\n",
      "   macro avg       0.85      0.75      0.78     30099\n",
      "weighted avg       0.87      0.87      0.86     30099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer,feature_rep_end, X_train, y_train = feature_rep(datasets_train[dataset_name])\n",
    "X_val, y_val = extractXy(datasets_test[dataset_name])\n",
    "model = train_eval_model(input_layer,feature_rep_end, X_train, y_train,X_val, y_val, batch_size=128, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6f1cb11c-9b57-43b7-91e6-7dcd95b38d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 28s 232ms/step\n",
      "Precision for Hate Class: 0.8189749182115594\n",
      "Recall for Hate Class: 0.8344444444444444\n",
      "F1 Macro 0.8869940707992034\n",
      "F1 Weighted 0.9194938627849395\n",
      "0.82 / 0.83 / 0.89 / 0.92\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      3000\n",
      "           1       0.82      0.83      0.83       900\n",
      "\n",
      "    accuracy                           0.92      3900\n",
      "   macro avg       0.88      0.89      0.89      3900\n",
      "weighted avg       0.92      0.92      0.92      3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = eval(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0ce3e9-28fb-47da-8252-25cfa46607ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"weights\\ensemble\\{dataset_name}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7846113-1002-4a30-8f66-e9ebf469dd9f",
   "metadata": {},
   "source": [
    "## Ensemble Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527ad9d3-3768-4fbd-9c7c-d5ec94c623a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 21s 226ms/step\n",
      "85/85 [==============================] - 20s 235ms/step\n",
      "10/10 [==============================] - 2s 216ms/step\n",
      "94/94 [==============================] - 22s 233ms/step\n",
      "122/122 [==============================] - 28s 234ms/step\n",
      "404/404 [==============================] - 100s 248ms/step\n"
     ]
    }
   ],
   "source": [
    "for key in datasets_train:\n",
    "    # # Combined Train\n",
    "    # X_train, y_train = extractXy(datasets_train[key])\n",
    "    # train_pred = model.predict(X_train)\n",
    "    # datasets_train[key][dataset_name] = train_pred[:,1]\n",
    "    # Combined Test\n",
    "    X_test, y_test = extractXy(datasets_test[key])\n",
    "    test_pred = model.predict(X_test)\n",
    "    datasets_test[key][dataset_name] = test_pred[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357f1409-c04a-4e8b-b10f-f2f6c376be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in datasets_train:\n",
    "    datasets_train[key].to_csv(f\"datasets/model_training/ensemble/{key}_ensemble_train2.csv\",header=True, index=False)\n",
    "    datasets_test[key].to_csv(f\"datasets/model_training/ensemble/{key}_ensemble_test2.csv\",header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8d68-4736-433c-ba00-dc2d4d33d98e",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd5d2e-7fca-40ba-bda5-196f011e1bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ccd920-9afa-4fca-bb32-fcad8715b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train = {\n",
    "    \"davidson\" : pd.read_csv(\"datasets\\model_training\\ensemble\\davidson_ensemble_train.csv\"),\n",
    "    \"hateval\" : pd.read_csv(\"datasets\\model_training\\ensemble\\hateval_ensemble_train.csv\"),\n",
    "    \"ethos\" : pd.read_csv(\"datasets\\model_training\\ensemble\\ethos_ensemble_train.csv\"),\n",
    "    \"jigsaw\": pd.read_csv(\"datasets\\model_training\\ensemble\\jigsaw_ensemble_train.csv\"),\n",
    "    \"qian\": pd.read_csv(\"datasets\\model_training\\ensemble\\qian_ensemble_train.csv\"),\n",
    "    \"combined\": pd.read_csv(\"datasets\\model_training\\ensemble\\combined_ensemble_train.csv\")\n",
    "}\n",
    "datasets_test = {\n",
    "    \"davidson\" : pd.read_csv(\"datasets\\model_training\\ensemble\\davidson_ensemble_test.csv\"),\n",
    "    \"hateval\" : pd.read_csv(\"datasets\\model_training\\ensemble\\hateval_ensemble_test.csv\"),\n",
    "    \"ethos\" : pd.read_csv(\"datasets\\model_training\\ensemble\\ethos_ensemble_test.csv\"),\n",
    "    \"jigsaw\": pd.read_csv(\"datasets\\model_training\\ensemble\\jigsaw_ensemble_test.csv\"),\n",
    "    \"qian\": pd.read_csv(\"datasets\\model_training\\ensemble\\qian_ensemble_test.csv\"),\n",
    "    \"combined\": pd.read_csv(\"datasets\\model_training\\ensemble\\combined_ensemble_test.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4946a308-c229-4bcb-85b4-559a8f21738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train_d = {\n",
    "    \"davidson\" : pd.read_csv(\"datasets\\model_training\\ensemble\\davidson_train.csv\"),\n",
    "    \"hateval\" : pd.read_csv(\"datasets\\model_training\\ensemble\\hateval_train.csv\"),\n",
    "    \"ethos\" : pd.read_csv(\"datasets\\model_training\\ensemble\\ethos_train.csv\"),\n",
    "    \"jigsaw\": pd.read_csv(\"datasets\\model_training\\ensemble\\jigsaw_train.csv\"),\n",
    "    \"qian\": pd.read_csv(\"datasets\\model_training\\ensemble\\qian_train.csv\"),\n",
    "    \"combined\": pd.read_csv(\"datasets\\model_training\\ensemble\\combined_train.csv\")\n",
    "}\n",
    "datasets_test_d = {\n",
    "    \"davidson\" : pd.read_csv(\"datasets\\model_training\\ensemble\\davidson_test.csv\"),\n",
    "    \"hateval\" : pd.read_csv(\"datasets\\model_training\\ensemble\\hateval_test.csv\"),\n",
    "    \"ethos\" : pd.read_csv(\"datasets\\model_training\\ensemble\\ethos_test.csv\"),\n",
    "    \"jigsaw\": pd.read_csv(\"datasets\\model_training\\ensemble\\jigsaw_test.csv\"),\n",
    "    \"qian\": pd.read_csv(\"datasets\\model_training\\ensemble\\qian_test.csv\"),\n",
    "    \"combined\": pd.read_csv(\"datasets\\model_training\\ensemble\\combined_test.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "975d3a80-ce8e-4ac1-bbe2-ae5743c59f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in datasets_train:\n",
    "    # Combined Train\n",
    "\n",
    "    datasets_train_d[key][dataset_name] = datasets_train[key][key]\n",
    "    # Combined Test\n",
    "    datasets_test_d[key][dataset_name] = datasets_test[key][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f21e1d6e-27a4-4df9-ae5d-25c78e9d8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train = datasets_train_d\n",
    "datasets_test = datasets_test_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d33c8125-83d2-458c-acd8-d2b6b2eae46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>davidson</th>\n",
       "      <th>hateval</th>\n",
       "      <th>ethos</th>\n",
       "      <th>jigsaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>When are people going to stop asking Why are t...</td>\n",
       "      <td>0.022557</td>\n",
       "      <td>0.462743</td>\n",
       "      <td>0.024089</td>\n",
       "      <td>0.001241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>What happened to rap This right here This is rap</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.240539</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.003826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They are forgetting Russell Crowe s performanc...</td>\n",
       "      <td>0.145440</td>\n",
       "      <td>0.035060</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.002382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>It s killing me seeing gays treated like that</td>\n",
       "      <td>0.961536</td>\n",
       "      <td>0.253825</td>\n",
       "      <td>0.269297</td>\n",
       "      <td>0.996753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Tell me where to ship this comfy electric chai...</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.024079</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.082555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1</td>\n",
       "      <td>The world would be better without white people...</td>\n",
       "      <td>0.894410</td>\n",
       "      <td>0.928982</td>\n",
       "      <td>0.014217</td>\n",
       "      <td>0.990977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1</td>\n",
       "      <td>Teaching my kids transgenders are anything but...</td>\n",
       "      <td>0.033007</td>\n",
       "      <td>0.194156</td>\n",
       "      <td>0.112746</td>\n",
       "      <td>0.830763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1</td>\n",
       "      <td>destroy islam</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>0.664770</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1</td>\n",
       "      <td>Islam is pig shit</td>\n",
       "      <td>0.665261</td>\n",
       "      <td>0.262201</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.996764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1</td>\n",
       "      <td>Feminism is just about women crying about the ...</td>\n",
       "      <td>0.185415</td>\n",
       "      <td>0.635512</td>\n",
       "      <td>0.087810</td>\n",
       "      <td>0.435493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                               text  davidson  \\\n",
       "0        0  When are people going to stop asking Why are t...  0.022557   \n",
       "1        0   What happened to rap This right here This is rap  0.003662   \n",
       "2        0  They are forgetting Russell Crowe s performanc...  0.145440   \n",
       "3        0      It s killing me seeing gays treated like that  0.961536   \n",
       "4        0  Tell me where to ship this comfy electric chai...  0.001930   \n",
       "..     ...                                                ...       ...   \n",
       "294      1  The world would be better without white people...  0.894410   \n",
       "295      1  Teaching my kids transgenders are anything but...  0.033007   \n",
       "296      1                                      destroy islam  0.006779   \n",
       "297      1                                  Islam is pig shit  0.665261   \n",
       "298      1  Feminism is just about women crying about the ...  0.185415   \n",
       "\n",
       "      hateval     ethos    jigsaw  \n",
       "0    0.462743  0.024089  0.001241  \n",
       "1    0.240539  0.001801  0.003826  \n",
       "2    0.035060  0.005101  0.002382  \n",
       "3    0.253825  0.269297  0.996753  \n",
       "4    0.024079  0.001312  0.082555  \n",
       "..        ...       ...       ...  \n",
       "294  0.928982  0.014217  0.990977  \n",
       "295  0.194156  0.112746  0.830763  \n",
       "296  0.664770  0.015431  0.000672  \n",
       "297  0.262201  0.859333  0.996764  \n",
       "298  0.635512  0.087810  0.435493  \n",
       "\n",
       "[299 rows x 6 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_test[\"ethos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b114cfe4-28d4-4f7a-9308-37303f323e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Hate Class: 0.8494055482166446\n",
      "Recall for Hate Class: 0.7144444444444444\n",
      "F1 Macro 0.8578537218619089\n",
      "F1 Weighted 0.901874209298717\n",
      "0.85 / 0.71 / 0.86 / 0.9\n"
     ]
    }
   ],
   "source": [
    "name = \"qian\"\n",
    "y_pred = np.round(datasets_test[name][\"combined\"].to_numpy())\n",
    "y_val = np.round(datasets_test[name][\"class\"].to_numpy())\n",
    "\n",
    "precision = precision_score(y_val, y_pred, average='binary')\n",
    "recall = recall_score(y_val, y_pred, average='binary')\n",
    "f1_macro_score = f1_score(y_val, y_pred, average='macro')\n",
    "f1_weighted_score = f1_score(y_val, y_pred, average='weighted')\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "print(\"Precision for Hate Class:\", precision)\n",
    "print(\"Recall for Hate Class:\", recall)\n",
    "print(\"F1 Macro\", f1_macro_score)\n",
    "print(\"F1 Weighted\", f1_weighted_score)\n",
    "print(round(precision,2), \"/\",round(recall,2), \"/\", round(f1_macro_score,2), \"/\", round(f1_weighted_score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1e343-ad00-4f05-8690-490cd5267926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
