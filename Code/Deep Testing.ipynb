{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1bef8c-70c9-4035-b14d-2741acb5bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806a1d65-04f5-4c59-9e83-b4e31b9b06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"davidson\" : pd.read_csv(\"literature\\datasets\\dt\\labeled_data_all_2classes_only.csv\", index_col=0),\n",
    "    \"hateval\" : pd.read_csv(\"literature\\\\datasets\\\\hateval\\\\train_en.tsv\", sep='\\t', index_col=0),\n",
    "    \"ethos\" : pd.read_csv(\"literature\\\\datasets\\\\ethos\\\\Ethos_Dataset_Binary.csv\", sep=';')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72976c4-1cad-427b-b2c8-6c3a6233b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"davidson\"] = datasets[\"davidson\"].rename({\"tweet\":\"text\"}, axis=1)\n",
    "datasets[\"davidson\"]['class'] = datasets[\"davidson\"]['class'].replace({0: 1, 2: 0})\n",
    "\n",
    "datasets[\"hateval\"] = datasets[\"hateval\"].rename({\"HS\":\"class\"}, axis=1)\n",
    "\n",
    "datasets[\"ethos\"] = datasets[\"ethos\"].rename({\"comment\":\"text\", \"isHate\":\"class\"}, axis=1)\n",
    "datasets[\"ethos\"]['class'] = datasets[\"ethos\"]['class'].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ddb2e82-d86f-4e56-9d9d-f1d53515640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    df_pm = data.lower()\n",
    "    return df_pm\n",
    "def sep_rem(data):\n",
    "    df_pm = data\n",
    "    df_pm = re.sub(r\"[^a-zA-Z0-9#@ ]\",\"\",df_pm)\n",
    "    return df_pm.strip()\n",
    "def remove_hashtag(data):\n",
    "    df_pm = re.sub(r\"#\\S+\", \"\", data)\n",
    "    return df_pm.strip()\n",
    "def remove_mentions(data):\n",
    "    data = re.sub(r\"@\\S+\", \"\", data)\n",
    "    return data.strip()\n",
    "def remove_NCR(data):\n",
    "    data = re.sub(r\"&#[0-9]+;|&#x[0-9a-fA-F]+;|&[a-zA-Z]+\", \"\", data)\n",
    "    return data.strip()\n",
    "def remove_RT(data):\n",
    "    data = re.sub(r\"(^|\\s)rt\\s\", \"\", data)\n",
    "    return data.strip()\n",
    "def remove_links(data):\n",
    "    data = re.sub(r\"https?://(?:[\\w./])+\", \" \", data)\n",
    "    data = re.sub(r\"http?://(?:[\\w./&#])+\", \" \", data)\n",
    "    return data.strip()\n",
    "def remove_spaces(data):\n",
    "    data = re.sub(r\" +\", \" \", data)\n",
    "    return data.strip()\n",
    "    \n",
    "\n",
    "def process_data(df):\n",
    "    processed_column = df['text']\n",
    "    processed_column = processed_column.apply(lambda x:pre_process(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_links(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_NCR(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_hashtag(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_mentions(x))\n",
    "    processed_column = processed_column.apply(lambda x:sep_rem(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_RT(x))\n",
    "    processed_column = processed_column.apply(lambda x:remove_spaces(x))\n",
    "    df[\"clean\"] = processed_column\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add77507-deb7-41fa-b66b-eafe93bb7941",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = api.load('word2vec-google-news-300')\n",
    "word_indices = list(pretrained_model.key_to_index.keys())\n",
    "word_freq = {word: pretrained_model.get_vecattr(word, \"count\") for word in word_indices}\n",
    "\n",
    "def text_to_vectors(text):\n",
    "    tokens = text.split()  \n",
    "    \n",
    "    # Get word vectors for each token\n",
    "    word_vectors = [pretrained_model[token] for token in tokens if token in pretrained_model]\n",
    "\n",
    "    if not word_vectors:\n",
    "        # Return zero-filled array of the desired shape\n",
    "        return np.zeros((100, 300), dtype=np.float32)\n",
    "    \n",
    "    # Pad sequence\n",
    "    max_length = 100\n",
    "    padded_vectors = np.zeros((max_length, 300), dtype=np.float32)\n",
    "    num_vectors = min(len(word_vectors), max_length)\n",
    "    padded_vectors[:num_vectors] = word_vectors[:max_length]\n",
    "    \n",
    "    return padded_vectors\n",
    "\n",
    "# Feature Representation\n",
    "def feature_rep(df, feature_type=\"TFIDF\", resample=True, split=0.3):\n",
    "    start_time = time.time()\n",
    "    if feature_type == \"TFIDF\": # TFIDF\n",
    "        documents = df[\"clean\"].tolist()\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(lowercase=False,ngram_range=(1, 3),max_features=10000,min_df=5,max_df=0.501)\n",
    "        X = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    elif feature_type == \"BOW\": # Bag of words\n",
    "        vectorizer = CountVectorizer()\n",
    "        documents = df[\"clean\"].tolist()\n",
    "    \n",
    "        bow_matrix = vectorizer.fit_transform(documents)\n",
    "        X = bow_matrix.toarray()\n",
    "    \n",
    "        vocabulary = vectorizer.get_feature_names_out()\n",
    "    elif feature_type == \"W2V\": # Word2Vec\n",
    "        # Tokenize\n",
    "        df['tokenized_text'] = df['clean'].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        # Apply text_to_vectors function to convert text to Word2Vec vectors\n",
    "        df['text_vector'] = df['clean'].apply(text_to_vectors)\n",
    "        \n",
    "        # Convert document vectors to numpy array\n",
    "        X = np.stack(df['text_vector'].values, axis=0)\n",
    "        y = pd.get_dummies(df['class']).values.astype(int)\n",
    "        input_layer = Input((100, 300))\n",
    "        # X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=split)\n",
    "        \n",
    "        \n",
    "        # if resample:\n",
    "        #     y_train = np.argmax(y_train, axis=1)\n",
    "            \n",
    "        #     num_classes = len(set(y_train.tolist()))\n",
    "            \n",
    "        #     num_samples, seq_length, feature_dim = X_train.shape\n",
    "            \n",
    "        #     X_flattened = X_train.reshape(num_samples, -1)  # Reshaping to (24783, 100*300)\n",
    "        #     oversampler = RandomOverSampler()\n",
    "        #     X_resampled, y_resampled = oversampler.fit_resample(X_flattened, y_train)\n",
    "            \n",
    "        #     X_original_shape = X_resampled.reshape(len(X_resampled), seq_length, feature_dim)\n",
    "        #     y_onehot = np.eye(num_classes)[y_resampled]\n",
    "        #     X_train, y_train = X_original_shape, y_onehot\n",
    "            \n",
    "    end_time = time.time()\n",
    "        \n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Feature Type:\", feature_type)\n",
    "    print(\"Feature Rep Elapsed time:\", round(elapsed_time,2), \"seconds\")\n",
    "    print(\"\")\n",
    "    return input_layer, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d27e07e-90d2-425c-bb50-584d5faa725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Embedding, Dense, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten, InputLayer, Input, Dropout, Concatenate\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd8e7cc-6fc3-4da8-a6f9-34fbb894fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(input_layer, X, y, model=\"complex CNN\", batch_size=100, epochs=30, num_folds=5, target_class=1):\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    all_precisions, all_recalls, all_f1_macro, all_f1_weighted = [], [], [], []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]  # y contains your labels\n",
    "\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        if model == \"complex CNN\":\n",
    "            input = input_layer\n",
    "            \n",
    "            conv_layers = []\n",
    "            kernel_sizes = [2, 3, 4]\n",
    "            for kernel_size in kernel_sizes:\n",
    "                conv_layer = Conv1D(filters=100, kernel_size=kernel_size, activation='relu')(input)\n",
    "                conv_layers.append(conv_layer)\n",
    "                \n",
    "            concat = Concatenate(axis=1)(conv_layers)\n",
    "            lstm = LSTM(32, return_sequences=True)(concat)\n",
    "            global_pool = GlobalMaxPooling1D()(lstm)\n",
    "            dense1 = Dense(16, activation='relu')(global_pool)\n",
    "            dense2 = Dense(2, activation='softmax')(dense1)\n",
    "            model = Model(inputs=input, outputs=dense2)\n",
    "        \n",
    "        # Estimate memory usage (assuming 32-bit float)\n",
    "        num_params = np.sum([np.prod(w.shape) for w in model.trainable_variables])\n",
    "        memory_usage = num_params * 4\n",
    "        print(\"Estimated model size (bytes):\", memory_usage)\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        print(\"Predict time:\", round(elapsed_time,7), \"seconds\")\n",
    "        \n",
    "        # Calculate precision and recall for the target class\n",
    "        precision = precision_score(y_val, y_pred, average='binary')\n",
    "        recall = recall_score(y_val, y_pred, average='binary')\n",
    "        f1_macro_score = f1_score(y_val, y_pred, average='macro')\n",
    "        f1_weighted_score = f1_score(y_val, y_pred, average='weighted')\n",
    "        \n",
    "        # Store precision and recall for this fold\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1_macro.append(f1_macro_score)\n",
    "        all_f1_weighted.append(f1_weighted_score)\n",
    "\n",
    "    avg_precision = sum(all_precisions) / num_folds\n",
    "    avg_recall = sum(all_recalls) / num_folds\n",
    "    avg_f1_macro = sum(all_f1_macro) / num_folds\n",
    "    avg_f1_weighted = sum(all_f1_weighted) / num_folds\n",
    "    \n",
    "    print(\"Average Precision for Class\", target_class, \":\", avg_precision)\n",
    "    print(\"Average Recall for Class\", target_class, \":\", avg_recall)\n",
    "    print(\"Average F1 Macro\", avg_f1_macro)\n",
    "    print(\"Average F1 Weighted\", avg_f1_weighted)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e27f338-013d-453e-b585-65627574b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  davidson\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = \"davidson\" # Options: davidson, hateval, ethos\n",
    "print(\"Dataset: \", dataset)\n",
    "print(\"\")\n",
    "\n",
    "df = datasets[dataset]\n",
    "df = process_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6fd4a6-be05-429b-8b54-cf6d9d2e7170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Type: W2V\n",
      "Feature Rep Elapsed time: 3.74 seconds\n",
      "\n",
      "(24783, 100, 300)\n",
      "(24783, 2)\n"
     ]
    }
   ],
   "source": [
    "input_layer, X, y = feature_rep(df, feature_type=\"W2V\", resample=True, split=0.3) \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3665e7-cf85-4227-ac97-fd41ad0c39bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated model size (bytes): 1151544\n",
      "Epoch 1/2\n",
      "661/661 [==============================] - 15s 19ms/step - loss: 0.1978 - accuracy: 0.9466 - val_loss: 0.2057 - val_accuracy: 0.9231\n",
      "Epoch 2/2\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1549 - accuracy: 0.9484 - val_loss: 0.1915 - val_accuracy: 0.9268\n",
      "155/155 [==============================] - 2s 9ms/step\n",
      "Predict time: 2.1379988 seconds\n",
      "Estimated model size (bytes): 1151544\n",
      "Epoch 1/2\n",
      "661/661 [==============================] - 12s 16ms/step - loss: 0.1398 - accuracy: 0.9507 - val_loss: 0.1919 - val_accuracy: 0.9264\n",
      "Epoch 2/2\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1254 - accuracy: 0.9541 - val_loss: 0.2058 - val_accuracy: 0.9280\n",
      "155/155 [==============================] - 1s 6ms/step\n",
      "Predict time: 1.4399993 seconds\n",
      "Estimated model size (bytes): 1151544\n",
      "Epoch 1/2\n",
      "661/661 [==============================] - 11s 15ms/step - loss: 0.1331 - accuracy: 0.9507 - val_loss: 0.0966 - val_accuracy: 0.9659\n",
      "Epoch 2/2\n",
      "661/661 [==============================] - 10s 15ms/step - loss: 0.1100 - accuracy: 0.9586 - val_loss: 0.1005 - val_accuracy: 0.9649\n",
      "155/155 [==============================] - 1s 6ms/step\n",
      "Predict time: 1.427001 seconds\n",
      "Estimated model size (bytes): 1151544\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_eval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplex CNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mtrain_eval_model\u001b[1;34m(input_layer, X, y, model, batch_size, epochs, num_folds, target_class)\u001b[0m\n\u001b[0;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml_env3.10\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml_env3.10\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "train_eval_model(input_layer, X, y, model=\"complex CNN\",batch_size=30,epochs=2,num_folds=4, target_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f11903f-dbeb-4f77-b761-ca0d4b74786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a39899-248f-40f2-9459-956330f4511b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
